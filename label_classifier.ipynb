{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import cv2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, Flatten, Merge, Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing import image, text\n",
    "from keras.utils import np_utils as u\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "K_FOLDS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pkl.load(open('./data/df_train.dump.pkl', 'rb'))\n",
    "df_valid = pkl.load(open('./data/df_valid.dump.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageId</th>\n",
       "      <th>labelId</th>\n",
       "      <th>taskId</th>\n",
       "      <th>object</th>\n",
       "      <th>attribute</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>dress</td>\n",
       "      <td>decoration</td>\n",
       "      <td>printed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>dress</td>\n",
       "      <td>color</td>\n",
       "      <td>purple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imageId labelId taskId object   attribute    label\n",
       "0       1       6      5  dress  decoration  printed\n",
       "1       2       7      6  dress       color   purple"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageId</th>\n",
       "      <th>labelId</th>\n",
       "      <th>taskId</th>\n",
       "      <th>object</th>\n",
       "      <th>attribute</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>shoe</td>\n",
       "      <td>gender</td>\n",
       "      <td>men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>shoe</td>\n",
       "      <td>age</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imageId labelId taskId object attribute  label\n",
       "0       1       1      1   shoe    gender    men\n",
       "1       1       2      2   shoe       age  adult"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2958 missed.\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "labels_train = []\n",
    "objects_train = []\n",
    "attributes_train = []\n",
    "missed = []\n",
    "for idx, rows in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "    img = cv2.imread(utility.image_location(rows['imageId'], config.RESIZED_TRAIN_DIR))\n",
    "    if type(img) != np.ndarray:\n",
    "        missed.append(rows['imageId'])\n",
    "        continue\n",
    "    h, w, c = img.shape\n",
    "    app_img = np.empty(shape=(config.MAX_PIXEL, config.MAX_PIXEL, 3), dtype=np.uint8)\n",
    "    app_img[:h, :w] = img\n",
    "    X_train.append(app_img)\n",
    "    labels_train.append(rows['label'].strip())\n",
    "    objects_train.append(rows['object'].strip())\n",
    "    attributes_train.append(rows['attribute'].strip())\n",
    "print(\"%d missed.\" % len(missed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "532 missed.\n"
     ]
    }
   ],
   "source": [
    "X_valid = []\n",
    "labels_valid = []\n",
    "objects_valid = []\n",
    "attributes_valid = []\n",
    "missed = []\n",
    "for idx, rows in tqdm(df_valid.iterrows(), total=len(df_valid)):\n",
    "    img = cv2.imread(utility.image_location(rows['imageId'], config.RESIZED_VALID_DIR))\n",
    "    if type(img) != np.ndarray:\n",
    "        missed.append(rows['imageId'])\n",
    "        continue\n",
    "    h, w, c = img.shape\n",
    "    app_img = np.empty(shape=(config.MAX_PIXEL, config.MAX_PIXEL, 3), dtype=np.uint8)\n",
    "    app_img[:h, :w] = img\n",
    "    X_valid.append(app_img)\n",
    "    labels_valid.append(rows['label'].strip())\n",
    "    objects_valid.append(rows['object'].strip())\n",
    "    attributes_valid.append(rows['attribute'].strip())\n",
    "print(\"%d missed.\" % len(missed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_valid = np.array(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer = text.Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels_train)\n",
    "y_train = label_tokenizer.texts_to_sequences(labels_train)\n",
    "y_valid = label_tokenizer.texts_to_sequences(labels_valid)\n",
    "object_tokenizer = text.Tokenizer()\n",
    "object_tokenizer.fit_on_texts(objects_train)\n",
    "obj_train = object_tokenizer.texts_to_sequences(objects_train)\n",
    "obj_valid = object_tokenizer.texts_to_sequences(objects_valid)\n",
    "attr_tokenizer = text.Tokenizer()\n",
    "attr_tokenizer.fit_on_texts(attributes_train)\n",
    "attr_train = attr_tokenizer.texts_to_sequences(attributes_train)\n",
    "attr_valid = attr_tokenizer.texts_to_sequences(attributes_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num labels: 356\n",
      "num objects: 4\n",
      "num attributes: 25\n"
     ]
    }
   ],
   "source": [
    "print(\"num labels:\", len(label_tokenizer.word_index))\n",
    "print(\"num objects:\", len(object_tokenizer.word_index))\n",
    "print(\"num attributes:\", len(attr_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([_[0] for _ in y_train])\n",
    "y_valid = np.array([_[0] for _ in y_valid])\n",
    "X_obj_train = to_categorical(np.array([_[0] for _ in obj_train]))\n",
    "X_obj_valid = to_categorical(np.array([_[0] for _ in obj_valid]))\n",
    "X_attr_train = to_categorical(np.array([_[0] for _ in attr_train]))\n",
    "X_attr_valid = to_categorical(np.array([_[0] for _ in attr_valid]))\n",
    "cls_wgt = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "cls_wgt = np.array([0] + cls_wgt.tolist())\n",
    "cls_wgt = {idx: value for idx, value in enumerate(cls_wgt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = X_train.astype('float32')/255.0, X_valid.astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "X_train: (52430, 32, 32, 3)\n",
      "X_obj_train: (52430, 5)\n",
      "X_attr_train: (52430, 26)\n",
      "y_train: (52430,)\n",
      "X_valid: (10473, 32, 32, 3)\n",
      "X_obj_valid: (10473, 5)\n",
      "X_attr_valid: (10473, 26)\n",
      "y_valid: (10473,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_obj_train:\", X_obj_train.shape)\n",
    "print(\"X_attr_train:\", X_attr_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_valid:\", X_valid.shape)\n",
    "print(\"X_obj_valid:\", X_obj_valid.shape)\n",
    "print(\"X_attr_valid:\", X_attr_valid.shape)\n",
    "print(\"y_valid:\", y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((X_train, X_valid))\n",
    "X_obj = np.vstack((X_obj_train, X_obj_valid))\n",
    "X_attr = np.vstack((X_attr_train, X_attr_valid))\n",
    "y = np.hstack((y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "X: (62903, 32, 32, 3)\n",
      "X_obj: (62903, 5)\n",
      "X_attr: (62903, 26)\n",
      "y: (62903,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\")\n",
    "print(\"X:\", X.shape)\n",
    "print(\"X_obj:\", X_obj.shape)\n",
    "print(\"X_attr:\", X_attr.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:16: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "img_model = Sequential()\n",
    "img_model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same',\n",
    "             activation='relu'))\n",
    "img_model.add(Dropout(0.2))\n",
    "img_model.add(Conv2D(32, (3, 3), activation='relu', padding='valid'))\n",
    "img_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "img_model.add(Flatten())\n",
    "\n",
    "obj_model = Sequential()\n",
    "obj_model.add(Dense(10, activation='relu', input_shape=(len(object_tokenizer.word_index)+1,)))\n",
    "\n",
    "attr_model = Sequential()\n",
    "attr_model.add(Dense(60, activation='relu', input_shape=(len(attr_tokenizer.word_index)+1,)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([img_model, obj_model, attr_model], mode='concat'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(label_tokenizer.word_index)+1, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer=SGD(momentum=0.5, decay=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = './models/object_model.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv2d_3_input:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'dense_5_input:0' shape=(?, 5) dtype=float32>,\n",
       " <tf.Tensor 'dense_6_input:0' shape=(?, 26) dtype=float32>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_8/Softmax:0' shape=(?, 357) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1/5...\n",
      "Train on 50199 samples, validate on 12704 samples\n",
      "Epoch 1/50\n",
      "50199/50199 [==============================] - 77s 2ms/step - loss: 1.8704 - acc: 0.4356 - val_loss: 1.6092 - val_acc: 0.5351\n",
      "Epoch 2/50\n",
      "50199/50199 [==============================] - 77s 2ms/step - loss: 1.8243 - acc: 0.4434 - val_loss: 1.6175 - val_acc: 0.5271\n",
      "Epoch 3/50\n",
      "50199/50199 [==============================] - 77s 2ms/step - loss: 1.7903 - acc: 0.4549 - val_loss: 1.6239 - val_acc: 0.5246\n",
      "Epoch 4/50\n",
      "50199/50199 [==============================] - 77s 2ms/step - loss: 1.7628 - acc: 0.4635 - val_loss: 1.6343 - val_acc: 0.5179\n",
      "Training on fold 2/5...\n",
      "Train on 50257 samples, validate on 12646 samples\n",
      "Epoch 1/50\n",
      "50257/50257 [==============================] - 77s 2ms/step - loss: 1.7722 - acc: 0.4597 - val_loss: 1.4498 - val_acc: 0.5935\n",
      "Epoch 2/50\n",
      "50257/50257 [==============================] - 77s 2ms/step - loss: 1.7333 - acc: 0.4697 - val_loss: 1.4631 - val_acc: 0.5845\n",
      "Epoch 3/50\n",
      "50257/50257 [==============================] - 77s 2ms/step - loss: 1.6970 - acc: 0.4803 - val_loss: 1.4786 - val_acc: 0.5685\n",
      "Epoch 4/50\n",
      "50257/50257 [==============================] - 77s 2ms/step - loss: 1.6645 - acc: 0.4909 - val_loss: 1.4814 - val_acc: 0.5636\n",
      "Training on fold 3/5...\n",
      "Train on 50312 samples, validate on 12591 samples\n",
      "Epoch 1/50\n",
      "50312/50312 [==============================] - 77s 2ms/step - loss: 1.6947 - acc: 0.4814 - val_loss: 1.3179 - val_acc: 0.6486\n",
      "Epoch 2/50\n",
      "50312/50312 [==============================] - 77s 2ms/step - loss: 1.6520 - acc: 0.4908 - val_loss: 1.3249 - val_acc: 0.6339\n",
      "Epoch 3/50\n",
      "50312/50312 [==============================] - 77s 2ms/step - loss: 1.6191 - acc: 0.5013 - val_loss: 1.3319 - val_acc: 0.6268\n",
      "Epoch 4/50\n",
      "50312/50312 [==============================] - 77s 2ms/step - loss: 1.5986 - acc: 0.5083 - val_loss: 1.3520 - val_acc: 0.6200\n",
      "Training on fold 4/5...\n",
      "Train on 50381 samples, validate on 12522 samples\n",
      "Epoch 1/50\n",
      "50381/50381 [==============================] - 77s 2ms/step - loss: 1.6192 - acc: 0.4970 - val_loss: 1.1721 - val_acc: 0.6997\n",
      "Epoch 2/50\n",
      "50381/50381 [==============================] - 77s 2ms/step - loss: 1.5823 - acc: 0.5104 - val_loss: 1.1871 - val_acc: 0.6876\n",
      "Epoch 3/50\n",
      "50381/50381 [==============================] - 77s 2ms/step - loss: 1.5497 - acc: 0.5203 - val_loss: 1.2052 - val_acc: 0.6740\n",
      "Epoch 4/50\n",
      "50381/50381 [==============================] - 77s 2ms/step - loss: 1.5269 - acc: 0.5276 - val_loss: 1.2196 - val_acc: 0.6636\n",
      "Training on fold 5/5...\n",
      "Train on 50463 samples, validate on 12440 samples\n",
      "Epoch 1/50\n",
      "50463/50463 [==============================] - 77s 2ms/step - loss: 1.5416 - acc: 0.5231 - val_loss: 1.0781 - val_acc: 0.7289\n",
      "Epoch 2/50\n",
      "50463/50463 [==============================] - 77s 2ms/step - loss: 1.5094 - acc: 0.5316 - val_loss: 1.0991 - val_acc: 0.7169\n",
      "Epoch 3/50\n",
      "50463/50463 [==============================] - 77s 2ms/step - loss: 1.4806 - acc: 0.5369 - val_loss: 1.1106 - val_acc: 0.7015\n",
      "Epoch 4/50\n",
      "50463/50463 [==============================] - 77s 2ms/step - loss: 1.4494 - acc: 0.5523 - val_loss: 1.1304 - val_acc: 0.6976\n"
     ]
    }
   ],
   "source": [
    "for idx, (train_indices, valid_indices) in enumerate(skf.split(X, y)):\n",
    "    print(\"Training on fold \" + str(idx+1) + \"/\" + str(K_FOLDS) + \"...\") \n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_obj_train, X_attr_train = X_obj[train_indices], X_attr[train_indices]\n",
    "    X_valid, y_valid = X[valid_indices], y[valid_indices]\n",
    "    X_obj_valid, X_attr_valid = X_obj[valid_indices], X_attr[valid_indices]\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_valid = to_categorical(y_valid)\n",
    "    model.fit(\n",
    "        [X_train, X_obj_train, X_attr_train], y_train, \n",
    "        epochs=50, \n",
    "        validation_data=([X_valid, X_obj_valid, X_attr_valid], y_valid), \n",
    "        shuffle=True, \n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        batch_size=BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12440/12440 [==============================] - 2s 198us/step\n",
      "Accuracy: 69.76\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f\" % (model.evaluate([X_valid, X_obj_valid, X_attr_valid], y_valid)[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(label_tokenizer, open('./models/object_model_label_tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    h, w, c = img.shape\n",
    "    ip = np.empty(shape=(1, config.MAX_PIXEL, config.MAX_PIXEL, 3))\n",
    "    ip[0, :h, :w] = img\n",
    "    prediction = model.predict(ip)\n",
    "    return prediction[:, 1:][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class('./resized_test/1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "resized_test = os.listdir(config.RESIZED_TEST_DIR)\n",
    "for _ in tqdm(resized_test, total=len(resized_test)):\n",
    "    img_path = config.RESIZED_TEST_DIR + \"/\" + _\n",
    "    all_rows.append([img_path] + predict_class(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.DataFrame(all_rows, columns = ['img_path', 'dress', 'outerwear', 'pants', 'shoe'])\n",
    "pkl.dump(df_test, open('df_test.pkl', 'wb'))\n",
    "df_test = pkl.load(open('df_test.pkl', 'rb'))\n",
    "df_test.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "5bf8e9fb21af4c339fa2d54f54da3b54": {
     "views": [
      {
       "cell_index": 6
      }
     ]
    },
    "d7cdd1408811417a8cbc64504f1187bb": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
